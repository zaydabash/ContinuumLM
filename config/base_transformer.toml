# Discrete Transformer baseline configuration
# Standard stacked Transformer layers (non-ODE) for comparison

[model]
d_model = 256
n_heads = 4
d_ff = 1024
vocab_size = 8000
max_seq_len = 128
is_neural_ode = false
ode_t0 = 0.0
ode_t1 = 1.0
ode_solver = "Tsit5"
n_layers = 4

[training]
batch_size = 16
seq_len = 128
num_steps = 10000
log_every = 100
eval_every = 1000
lr = 1e-3
weight_decay = 0.01
grad_clip = 1.0
warmup_steps = 500
device = "auto"
checkpoint_dir = "checkpoints"
checkpoint_every = 1000
save_best = true
seed = 42

[data]
corpus_path = "data/corpus.txt"
tokenizer_path = "data/tokenizer.json"
vocab_size = 8000
train_split = 0.9

